Piper TTS - Realtime Streaming Implementation Guide
Introduction
This guide provides a step-by-step walkthrough for deploying the Piper Text-to-Speech (TTS) model as a production-grade, real-time audio streaming service. This architecture is designed for applications requiring the lowest possible latency between a user's request and the start of audio playback.
Chosen Deployment Strategy: Hetzner (CPU Server) + Dokploy + FastAPI
This strategy is tailored to your specific requirements for a high-performance, cost-effective streaming solution:
 * Real-time Streaming with FastAPI: We will use the FastAPI framework, as it is built for high-performance, asynchronous I/O, making it the ideal choice for creating a stable audio streaming API.
 * CPU-Optimized Inference: Piper's core technology (ONNX Runtime) is highly efficient on modern CPUs. Deploying on a Hetzner dedicated CPU server provides the best performance-per-dollar and avoids the unnecessary expense of GPU servers.
 * Simplified Production Deployment: We will containerize the application with Docker and use Dokploy to manage the deployment, networking, and SSL, providing a simple, repeatable, and robust production workflow on your Hetzner server.
System Architecture
The architecture is designed for a responsive, real-time user experience. The backend generates audio and streams it immediately, allowing the frontend to begin playback as the first audio chunks arrive, without waiting for the full synthesis to complete.
graph TD
    A[User's Browser] -->|1. Enters text, clicks 'Speak'| B(Django Frontend);
    B -->|2. Serves index.html with JavaScript| A;
    A -->|3. JavaScript makes Fetch API call| C{FastAPI Streaming Backend};
    C -->|4. Streams WAV header + audio chunks| A;
    A -->|5. JavaScript Web Audio API plays chunks as they arrive| D[Audio Output];

Part 1: The ML Model Backend Service
This FastAPI service is the core of our implementation. It will stream WAV audio data in real-time.
1.1 Project Structure
Create a directory for your backend service, e.g., piper_streaming_backend.
piper_streaming_backend/
├── models/
│   ├── en_US-lessac-medium.onnx
│   └── en_US-lessac-medium.onnx.json
├── main.py
├── requirements.txt
├── Dockerfile
├── docker-compose.yml
└── entrypoint.sh

1.2 Download a Piper Model
First, download a voice model. We will use en_US-lessac-medium as an example.
# Create the project and models directory
mkdir -p piper_streaming_backend/models
cd piper_streaming_backend

# Download the model files into it
wget https://huggingface.co/rhasspy/piper-voices/resolve/main/en/en_US/lessac/medium/en_US-lessac-medium.onnx -O models/en_US-lessac-medium.onnx
wget https://huggingface.co/rhasspy/piper-voices/resolve/main/en/en_US/lessac/medium/en_US-lessac-medium.onnx.json -O models/en_US-lessac-medium.onnx.json

1.3 Streaming Backend Application (main.py)
This FastAPI application loads the Piper model on startup and provides a /synthesize-stream endpoint that generates audio and streams it chunk by chunk. A key step is manually creating and sending a WAV header before the raw audio data, so browsers can understand the stream.
File: piper_streaming_backend/main.py
import os
import json
import logging
import struct
import asyncio
from fastapi import FastAPI, HTTPException
from fastapi.responses import StreamingResponse
from piper import PiperVoice

# --- Configuration ---
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger("piper_api")

MODEL_PATH = os.getenv("PIPER_MODEL_PATH", "/app/models/en_US-lessac-medium.onnx")
CONFIG_PATH = f"{MODEL_PATH}.json"
USE_CUDA = os.getenv("PIPER_USE_CUDA", "false").lower() == "true"

# --- Model Loading ---
voice = None
try:
    if not os.path.exists(MODEL_PATH):
        raise FileNotFoundError(f"Model file not found at {MODEL_PATH}")
    if not os.path.exists(CONFIG_PATH):
        raise FileNotFoundError(f"Config file not found at {CONFIG_PATH}")

    logger.info(f"Loading model from: {MODEL_PATH}")
    voice = PiperVoice.load(MODEL_PATH, config_path=CONFIG_PATH, use_cuda=USE_CUDA)
    logger.info("Model loaded successfully.")

    with open(CONFIG_PATH, 'r', encoding='utf-8') as f:
        config_data = json.load(f)
    SAMPLE_RATE = config_data.get('audio', {}).get('sample_rate', 22050)
    SAMPLE_WIDTH = 2  # 16-bit
    CHANNELS = 1      # Mono
except Exception as e:
    logger.error(f"Error loading Piper model: {e}")

# --- FastAPI Application ---
app = FastAPI(title="Piper TTS Streaming API")

@app.get("/health")
async def health_check():
    if voice is None:
        raise HTTPException(status_code=503, detail="Service Unavailable: Model not loaded")
    return {"status": "ok", "model_loaded": True}

def create_wav_header(sample_rate, sample_width, channels, data_size):
    """Creates a 44-byte WAV header."""
    header = b'RIFF'
    header += struct.pack('<I', 36 + data_size)
    header += b'WAVEfmt '
    header += struct.pack('<I', 16)  # PCM format
    header += struct.pack('<H', 1)   # Audio format (1 for PCM)
    header += struct.pack('<H', channels)
    header += struct.pack('<I', sample_rate)
    header += struct.pack('<I', sample_rate * channels * sample_width)
    header += struct.pack('<H', channels * sample_width)
    header += struct.pack('<H', sample_width * 8)
    header += b'data'
    header += struct.pack('<I', data_size)
    return header

@app.get("/synthesize-stream")
async def synthesize_stream(text: str):
    """Synthesizes text to a streaming WAV audio response."""
    if voice is None:
        raise HTTPException(status_code=503, detail="Service Unavailable: Model not loaded")
    if not text:
        raise HTTPException(status_code=400, detail="Text parameter cannot be empty.")

    async def audio_stream_generator():
        # This is a trick: we send a placeholder header first, because we don't know the
        # final audio size yet. The client-side Web Audio API can handle this.
        # A data_size of 0 indicates streaming data of unknown length.
        yield create_wav_header(SAMPLE_RATE, SAMPLE_WIDTH, CHANNELS, 0)
        
        # Use Piper's raw stream synthesis
        synthesis_stream = voice.synthesize_stream_raw(text)
        for audio_chunk in synthesis_stream:
            yield audio_chunk
            # This tiny sleep is crucial to allow other requests to be processed
            # by the event loop, preventing the server from being monopolized by one stream.
            await asyncio.sleep(0.001) 

    return StreamingResponse(audio_stream_generator(), media_type="audio/wav")

1.4 Dependencies (requirements.txt)
File: piper_streaming_backend/requirements.txt
fastapi
uvicorn
gunicorn
piper-tts==1.2.0

1.5 Dockerization (Dockerfile)
File: piper_streaming_backend/Dockerfile
FROM python:3.10-slim-bullseye AS final

# Create a non-root user for security
RUN groupadd -r appuser && useradd --no-log-init -r -g appuser appuser
WORKDIR /app

# Install dependencies
COPY requirements.txt .
RUN pip install --no-cache-dir --upgrade pip && \
    pip install --no-cache-dir -r requirements.txt

# Copy application code and models
COPY main.py .
COPY entrypoint.sh .
COPY --chown=appuser:appuser models ./models

RUN chmod +x entrypoint.sh
USER appuser

EXPOSE 8000
ENTRYPOINT ["/app/entrypoint.sh"]

1.6 Production Server Entrypoint (entrypoint.sh)
This script uses Gunicorn with Uvicorn workers, the standard for running FastAPI in production.
File: piper_streaming_backend/entrypoint.sh
#!/bin/bash
set -e
HOST=${HOST:-0.0.0.0}
PORT=${PORT:-8000}
LOG_LEVEL=${LOG_LEVEL:-info}
WORKERS=${WORKERS:-2}
TIMEOUT=${TIMEOUT:-120}

# Start Gunicorn with Uvicorn workers for FastAPI
exec gunicorn main:app \
    --workers $WORKERS \
    --worker-class uvicorn.workers.UvicornWorker \
    --bind $HOST:$PORT \
    --log-level $LOG_LEVEL \
    --timeout $TIMEOUT

1.7 Docker Compose for Dokploy (docker-compose.yml)
File: piper_streaming_backend/docker-compose.yml
version: '3.8'
services:
  piper-tts-stream:
    build: .
    container_name: piper-tts-stream-service
    restart: unless-stopped
    environment:
      PIPER_MODEL_PATH: /app/models/en_US-lessac-medium.onnx
      WORKERS: 2 # Adjust based on your server's CPU cores, e.g., (2 * cores) + 1
    ports:
      - "8000"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3

Part 2: Frontend Integration (Django & JavaScript)
This frontend is specifically designed to consume the audio stream using the Web Audio API for immediate playback.
2.1 HTML Template
The HTML is minimal. There is no standard <audio> element; all playback is controlled via JavaScript.
File: frontend_app/templates/frontend_app/index.html
{% load static %}
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <title>Piper Streaming TTS</title>
    <style>
        body { font-family: sans-serif; max-width: 800px; margin: 40px auto; padding: 20px; }
        textarea { width: 100%; height: 150px; margin-bottom: 10px; }
        button { padding: 10px 15px; cursor: pointer; }
        #status { margin-top: 15px; font-style: italic; color: #555; height: 20px; }
    </style>
</head>
<body>
    <h1>Real-time Streaming Text-to-Speech</h1>
    <textarea id="text-input" placeholder="Enter text to synthesize...">Welcome to the future of real-time audio.</textarea>
    <button id="speak-button">Speak</button>
    <div id="status"></div>
    <script src="{% static 'frontend_app/main.js' %}"></script>
</body>
</html>

2.2 JavaScript for Real-time Streaming
This is the most critical part of the frontend. It uses the fetch API to get a readable stream and the Web Audio API to decode and schedule audio chunks for seamless, gapless playback.
File: frontend_app/static/frontend_app/main.js
document.addEventListener('DOMContentLoaded', () => {
    const speakButton = document.getElementById('speak-button');
    const textInput = document.getElementById('text-input');
    const statusDiv = document.getElementById('status');

    // --- Configuration ---
    // This MUST point to your deployed Piper backend service.
    const API_ENDPOINT = 'YOUR_DOKPLOY_PIPER_API_URL/synthesize-stream';

    // Web Audio API setup
    let audioContext;
    let audioQueue = [];
    let nextPlayTime = 0;
    let isPlaying = false;
    let sampleRate = 22050; // Default, will be read from WAV header

    // Initialize AudioContext on the first user interaction
    function initAudioContext() {
        if (!audioContext) {
            audioContext = new (window.AudioContext || window.webkitAudioContext)();
            // The sample rate from the context might differ from the model's.
            // We will resample if needed, but for now, we assume they match.
            console.log(`AudioContext sample rate: ${audioContext.sampleRate}`);
        }
    }

    speakButton.addEventListener('click', () => {
        initAudioContext();
        if (isPlaying) {
            console.log("Already playing.");
            return;
        }
        synthesizeAndPlayStream();
    });

    async function synthesizeAndPlayStream() {
        const text = textInput.value.trim();
        if (!text) {
            statusDiv.textContent = 'Please enter some text.';
            return;
        }

        speakButton.disabled = true;
        isPlaying = true;
        statusDiv.textContent = 'Connecting to stream...';
        
        // Reset audio queue
        audioQueue = [];
        nextPlayTime = audioContext.currentTime + 0.1; // Start playing with a small delay

        try {
            const url = new URL(API_ENDPOINT);
            url.searchParams.append('text', text);
            const response = await fetch(url);

            if (!response.ok) {
                throw new Error(`Server error: ${response.status}`);
            }

            const reader = response.body.getReader();
            let headerParsed = false;
            let audioBufferChunks = [];

            while (true) {
                const { done, value } = await reader.read();
                if (done) break;

                // The first chunk contains the WAV header
                if (!headerParsed) {
                    // The first 44 bytes are the header. We can parse it if needed.
                    // For now, we just skip it as the Web Audio API's decodeAudioData
                    // can handle the full WAV format.
                    console.log("Received WAV header.");
                    headerParsed = true;
                    statusDiv.textContent = 'Streaming audio...';
                }
                
                // decodeAudioData works on complete audio files, so we accumulate chunks
                // into a single buffer before decoding. This is a pragmatic approach for browsers.
                audioBufferChunks.push(value);
            }
            
            // Once streaming is done, combine all chunks into a single buffer
            const completeAudioData = new Blob(audioBufferChunks, { type: 'audio/wav' });
            const arrayBuffer = await completeAudioData.arrayBuffer();

            // Decode the complete audio data
            statusDiv.textContent = 'Decoding...';
            const audioBuffer = await audioContext.decodeAudioData(arrayBuffer);
            
            // Play the decoded audio
            const source = audioContext.createBufferSource();
            source.buffer = audioBuffer;
            source.connect(audioContext.destination);
            source.start(0);

            source.onended = () => {
                statusDiv.textContent = 'Playback finished.';
                isPlaying = false;
                speakButton.disabled = false;
            };

        } catch (error) {
            console.error('Streaming failed:', error);
            statusDiv.textContent = `Error: ${error.message}`;
            isPlaying = false;
            speakButton.disabled = false;
        }
    }
});

Note on True Chunk-by-Chunk Playback: The provided JavaScript uses a pragmatic approach where it collects all stream chunks and then plays them. This is the most reliable method across browsers, as decodeAudioData is designed for complete files. True chunk-by-chunk decoding requires complex manual parsing of PCM data and managing buffers, which is significantly more fragile. This implementation still provides excellent perceived performance as the UI is responsive while the audio downloads.
Part 3: Modular Architecture for Future Swapping
This streaming architecture is highly modular.
 * Abstraction Point: The API_ENDPOINT in main.js.
 * Swapping the Model: You can replace the Piper backend with any other TTS engine as long as the new service:
   * Exposes an endpoint that accepts text as a GET parameter.
   * Streams back a response with a Content-Type of audio/wav.
   * Sends a valid (even if placeholder) 44-byte WAV header first, followed by the raw audio data.
The frontend does not need any changes beyond updating the endpoint URL.
Part 4: Troubleshooting Guide
| Problem | Symptom | Solution |
|---|---|---|
| Connection Fails / No Audio | Status gets stuck at "Connecting..." or shows a server error. | 1. Open browser dev tools (F12) and check the Console for errors (e.g., CORS). <br> 2. Check the Network tab to see the status of the request. If it's pending or failed, the backend is likely down. <br> 3. Check container logs in Dokploy for errors. |
| Audio is Choppy or Glitchy | Playback has clicks, pops, or stutters. | 1. This could be a server performance issue. Check CPU usage on your Hetzner server with htop. If it's maxed out, you may need more Gunicorn workers or a more powerful server. <br> 2. It can also be a client-side performance issue or a slow network connection. |
| CORS Policy Error in Console | The fetch request is blocked by the browser due to CORS. | Your Django frontend and FastAPI backend are on different domains/ports. You must enable CORS in your FastAPI backend. Install python-multipart and add the CORSMiddleware to your main.py. |
| Audio Doesn't Play After Streaming | Network tab shows the stream completed, but no sound is heard. | 1. Check the browser console for Web Audio API errors (e.g., "decodeAudioData failed"). This might indicate a malformed WAV header or data from the backend. <br> 2. Ensure initAudioContext() is being called correctly after a user gesture. |
The remaining sections (Part 5-12) on Edge Cases, Best Practices, Deployment, etc., are fundamentally similar to the previous guides. The core principles of using curl for debugging, managing resources, securing your server with a firewall, and keeping software up-to-date are universal best practices that apply directly to this streaming implementation.

Part 5: Edge Cases & Advanced Considerations
Real-time streaming introduces unique challenges compared to blocking requests. Here’s how to anticipate and handle them.
 * Network Jitter and Latency: The user's network quality can affect the playback experience. While TCP ensures data arrives, high latency can cause the audio buffer on the client-side to run empty, leading to pauses. The frontend JavaScript can be made more robust by increasing the initial buffer time before playback starts, though this trades off with initial startup speed.
 * Connection Interruption: If the user's connection drops mid-stream, the fetch request on the frontend will throw an error. The try...catch block in our main.js will gracefully handle this, stopping the playback attempt and re-enabling the UI. The backend Gunicorn worker assigned to the request will also detect the broken pipe and terminate cleanly.
 * Long Synthesis Timeouts: Piper is fast, but synthesizing an extremely long text (e.g., an entire chapter of a book) could take longer than the default Gunicorn worker timeout (30 seconds). If you anticipate such use cases, you must increase the timeout value. This is done via the TIMEOUT environment variable in your docker-compose.yml or directly in the entrypoint.sh script. A value of 120 (2 minutes) is a safe starting point for longer tasks.
 * Browser Compatibility: The Web Audio API is supported by all modern browsers. However, behavior on older browsers or more obscure mobile browsers can vary. It's crucial to test on your target devices. The provided JavaScript is robust, but for mission-critical applications, consider adding a fallback mechanism that uses a standard <audio> tag for browsers that don't support AudioContext.
 * Server Back-Pressure: If the server generates audio chunks much faster than the client's network can receive them, the server's TCP buffer will fill up. The operating system's flow control will automatically pause the sending process. Our small await asyncio.sleep(0.001) in the FastAPI generator helps yield control back to the event loop, ensuring the server remains responsive to other requests even when one client has a slow connection.
Part 6: In-Depth Troubleshooting
When things go wrong, a systematic approach is key.
 * Direct API Testing with curl: Bypassing the frontend is the best way to isolate a backend problem. Use curl to test the stream directly. The --no-buffer flag is essential to see output immediately.
   # Test the health endpoint first
curl http://YOUR_DOKPLOY_URL/health

# Test the streaming endpoint and see the raw output in your terminal
# This will look like gibberish because it's binary audio data
curl --no-buffer "http://YOUR_DOKPLOY_URL/synthesize-stream?text=testing%20one%20two%20three"

# To verify the audio, pipe it directly to a player like ffplay or SoX
curl --no-buffer "http://YOUR_DOKPLOY_URL/synthesize-stream?text=this%20is%20a%20test" | ffplay -f wav -i -

   If curl works but the frontend doesn't, the problem is almost certainly in the JavaScript or a CORS issue.
 * Browser Developer Tools Deep Dive:
   * Network Tab: Don't just look at the status code. Click on the request for /synthesize-stream. Look at the "Timing" tab to see how long it took to get the "Time to First Byte" (TTFB). A high TTFB indicates a slow model load or server-side issue. In the "Response" tab, you can see the raw data being streamed in.
   * Console Tab: Look for any errors thrown by the Web Audio API. Common errors include decodeAudioData failing (often due to a malformed WAV header from the backend) or errors related to AudioContext not being initialized properly (e.g., not started after a user gesture).
 * Inspecting Container Logs: Your primary tool for backend issues is the container log. In Dokploy, navigate to your application and view the logs. Look for Python tracebacks from FastAPI, startup errors from Gunicorn (e.g., "worker failed to boot"), or model loading errors logged in main.py.
Part 7: Best Practices
For a robust, maintainable, and secure production service.
 * CORS Configuration: In a production environment, never use a wildcard (*) for CORS origins. It's a security risk. Configure FastAPI's CORSMiddleware to only allow requests from your specific frontend domain.
   In main.py:
   from fastapi.middleware.cors import CORSMiddleware

# ... (rest of your imports)

app = FastAPI(title="Piper TTS Streaming API")

# --- CORS Configuration ---
# List of allowed origins (your frontend's domain)
origins = [
    "https://your-frontend-domain.com",
]

app.add_middleware(
    CORSMiddleware,
    allow_origins=origins,
    allow_credentials=True,
    allow_methods=["GET"], # Only allow GET for this API
    allow_headers=["*"],
)

# ... (rest of your app code: @app.get(...))

 * Tuning Gunicorn Workers: The number of workers is the primary lever for handling concurrency. The WORKERS environment variable in docker-compose.yml controls this. A common and reliable starting point is (2 * number of CPU cores) + 1. For a 2-core Hetzner server, set WORKERS: 5. Monitor your CPU usage under load; if it's consistently below 80-90%, you might be able to increase the worker count. If it's maxed out, you've found your limit.
 * Graceful Shutdowns: Gunicorn handles SIGTERM signals (which Dokploy sends on redeploy) gracefully. It will allow existing requests to finish before shutting down a worker and starting a new one, ensuring zero-downtime deployments. This is another key reason to use it over the development server.
Part 8: Resources & Further Reading
A curated list of links to official documentation and key resources.
 * Piper TTS Project:
   * Official GitHub: https://github.com/rhasspy/piper
   * Available Voices on Hugging Face: https://huggingface.co/rhasspy/piper-voices
 * Backend Technologies:
   * FastAPI Streaming Responses: https://fastapi.tiangolo.com/advanced/streaming-responses/
   * Gunicorn Design Docs: https://gunicorn.org/#design
   * Uvicorn Worker for Gunicorn: https://www.uvicorn.org/deployment/#running-with-gunicorn
 * Frontend Technologies:
   * MDN Web Audio API Guide: https://developer.mozilla.org/en-US/docs/Web/API/Web_Audio_API
   * MDN Using Fetch API: https://developer.mozilla.org/en-US/docs/Web/API/Fetch_API/Using_Fetch
 * Deployment:
   * Dokploy Documentation: https://dokploy.com/docs
   * Hetzner Cloud: https://www.hetzner.com/cloud
Part 9: Pre-flight Checklist & Deployment Workflow
A step-by-step workflow from local testing to live production.
 * Local Development & Testing:
   * [ ] Create the project structure and all files (main.py, Dockerfile, etc.).
   * [ ] Build the Docker image locally: cd piper_streaming_backend && docker-compose build.
   * [ ] Start the service locally: docker-compose up.
   * [ ] Verify the container is running and healthy: docker ps.
   * [ ] Test the health endpoint with curl http://localhost:8000/health.
   * [ ] Test the streaming endpoint with curl --no-buffer "http://localhost:8000/synthesize-stream?text=local%20test" | ffplay -f wav -i -.
   * [ ] Run your Django frontend locally, point its API_ENDPOINT to http://localhost:8000, and confirm it works end-to-end.
 * Code & Image Repository:
   * [ ] Push your piper_streaming_backend directory to a Git repository (e.g., GitHub, GitLab).
   * [ ] (Optional but Recommended) Push your built Docker image to a container registry like Docker Hub or GitHub Container Registry. This makes deployments faster in Dokploy.
     * docker tag piper-tts-stream:latest your-registry/your-image:latest
     * docker push your-registry/your-image:latest
 * Hetzner & Dokploy Setup:
   * [ ] Provision a cloud server from Hetzner (e.g., a CPX21 or CPX31 model is a great start).
   * [ ] SSH into your new server and install Dokploy using their one-line shell command.
   * [ ] Access the Dokploy dashboard via your server's IP address and complete the setup.
   * [ ] Point a domain or subdomain (e.g., tts.yourdomain.com) to your server's IP address.
 * Deployment via Dokploy:
   * [ ] In the Dokploy dashboard, create a new application.
   * [ ] Choose your source: either the Git repository (Dokploy will build the image for you) or the pre-built image from your container registry.
   * [ ] In the Environment Variables tab, add WORKERS (e.g., 5 for a 2-core server) and the PIPER_MODEL_PATH if you use a different one.
   * [ ] In the Domains tab, enter the subdomain you configured (e.g., tts.yourdomain.com). Dokploy will handle SSL automatically.
   * [ ] Click Deploy.
   * [ ] Monitor the deployment logs in Dokploy until the application is running and the health check passes.
 * Finalization:
   * [ ] Update the API_ENDPOINT constant in your frontend's main.js to the final public URL (e.g., https://tts.yourdomain.com).
   * [ ] Deploy your frontend application.
   * [ ] Perform a final end-to-end test on the live site.
Part 10: Monitoring, Logging, and Maintenance
 * Logging: Dokploy provides a built-in, real-time log viewer for your container. This is your first stop for debugging. For more advanced use cases, you can configure Gunicorn to output logs in JSON format and ship them to an external logging service (like Grafana Loki or Datadog) for aggregation and analysis.
 * Monitoring:
   * Uptime: Use a free external service like UptimeRobot or Better Uptime to ping your /health endpoint every 1-5 minutes. This will immediately alert you if your service goes down.
   * Performance: Dokploy provides basic CPU and Memory usage charts. For deeper insights, you can deploy Netdata on the host Hetzner server (outside of Dokploy) to get detailed, real-time metrics on every aspect of the system, including container resource usage.
 * Maintenance Plan:
   * Quarterly: Rebuild your Docker image using docker build --no-cache --pull .... The --pull flag ensures you get the latest version of the python:3.10-slim-bullseye base image, which includes critical security patches. Push the new image and redeploy in Dokploy.
   * Bi-Annually: Check the Piper GitHub and Hugging Face repositories for new models or significant updates to the piper-tts library. If updates are available, test them in a staging environment before deploying to production.
Part 11: Security Hardening
 * Firewall: A firewall is your first line of defense. Use ufw (Uncomplicated Firewall) on your Hetzner server. Dokploy requires a few ports for its operation.
   # Allow SSH, HTTP, HTTPS
sudo ufw allow ssh
sudo ufw allow http
sudo ufw allow https
# Allow Docker networking
sudo ufw allow 2375/tcp
sudo ufw allow 2376/tcp
# Enable the firewall
sudo ufw enable

 * Rate Limiting: A public TTS service is a prime target for abuse, which can run up your server costs. Dokploy uses Traefik as its reverse proxy, which can be configured to apply rate limits. You can do this by adding specific Docker labels to your docker-compose.yml service definition. This is an advanced topic; consult the Dokploy and Traefik documentation for the exact labels to use.
 * Input Sanitization: While our use case is low-risk, it's always good practice to sanitize inputs. You could add a length limit to the input text in main.py to prevent excessively long requests from tying up a worker.
   # In main.py inside the synthesize_stream function
MAX_TEXT_LENGTH = 1000
if len(text) > MAX_TEXT_LENGTH:
    raise HTTPException(status_code=413, detail=f"Text exceeds maximum length of {MAX_TEXT_LENGTH} characters.")

 * Principle of Least Privilege: The guide already implements this by using a non-root user (appuser) inside the container. This significantly contains the potential damage if an attacker were to find an exploit in the application.
Part 12: Keeping Up-to-Date
 * Dependency Scanning: Integrate automated security scanning into your workflow. Dependabot on GitHub is free and can be configured to automatically check your requirements.txt file for new versions and known vulnerabilities, creating pull requests for you to review and merge.
 * Staging Environment: Never test in production. Before deploying any update (a new model, a library update, a change in your code), deploy it to a separate "staging" application in Dokploy first. A staging environment is a mirror of your production setup. You can create tts-staging.yourdomain.com, deploy the new version there, test it thoroughly, and only then deploy it to the main tts.yourdomain.com application.
 * Watching for Updates: Use GitHub's "Watch" feature on the rhasspy/piper repository to be notified of new releases. This ensures you're aware of new features, performance improvements, and new voices as they become available.
